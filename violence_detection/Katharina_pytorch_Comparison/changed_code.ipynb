{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEY MODIFICATIONS\n",
    "\n",
    "1. Load the three specific images manually using a file path \n",
    "2. Prepocess the images using the \"prepocess\" function from CLIP \n",
    "3. Compute the embeddings for these images using the model\n",
    "4. Compute the similarity scores against the predefined text features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the 3 specific JPEG images\n",
    "image_paths = [\"path/to/image1.jpg\", \"path/to/image2.jpg\", \"path/to/image3.jpg\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model\n",
    "model_name = \"ViT-L/14\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, preprocess = clip.load(model_name, device=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define safety-related text prompts\n",
    "text_prompts = [\n",
    "    \"a photo of a gun\", \"a photo of a shooting\", \"a photo of a weapon\",\n",
    "    \"a photo of a killing\", \"a photo of a bomb\", \"a photo of violence\",\n",
    "    \"a photo of other\", \"a photo of an animal\", \"a photo of a family\",\n",
    "    \"a photo of a meeting\", \"a photo of a tool\", \"a photo of a gathering\"\n",
    "]\n",
    "text_tokens = clip.tokenize(text_prompts).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute text embeddings\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens).to(torch.float32)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and evaluate the images\n",
    "final_sim = {}\n",
    "for image_path in image_paths:\n",
    "    try:\n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Compute image embedding\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image_input).to(torch.float32)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Compute similarity with text features\n",
    "        similarity = image_features @ text_features.T\n",
    "\n",
    "        # Store the results\n",
    "        final_sim[image_path] = similarity.cpu().numpy()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "torch.save(final_sim, \"final_res/sim_violence_test.torch\")\n",
    "\n",
    "# Print results\n",
    "for image_path, sim in final_sim.items():\n",
    "    print(f\"Similarity scores for {image_path}: {sim}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
