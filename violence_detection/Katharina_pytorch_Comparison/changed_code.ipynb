{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEY MODIFICATIONS\n",
    "\n",
    "1. Load the three specific images manually using a file path \n",
    "2. Prepocess the images using the \"prepocess\" function from CLIP \n",
    "3. Compute the embeddings for these images using the model\n",
    "4. Compute the similarity scores against the predefined text features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erkl√§rungen \n",
    "\n",
    "1. Specific Image Paths: The Image_paths list contains the file paths of the three JPEG images you wnar to test \n",
    "2. Image oading: Each image is loaded using PIL.IMAGe and converted to RGB to ensure compatability with CLIP \n",
    "3. Prpeocessing: Each image is preprocessed sing the preprocess function from CLIP to prepare it for the model \n",
    "4. Embedding and Similarity Calculation: The image embedding is calcualted, normlaized and compared against the text emebddings using the dot product \n",
    "5. Save Results: The similarity results are saved in a final_res/sim_violence_test.torch file for later use\n",
    "6. Error Handling: Added a try-except block to handle potential issues like missing files or unsupported image formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the 3 specific JPEG images\n",
    "#image_paths = [\"path/to/image1.jpg\", \"path/to/image2.jpg\", \"path/to/image3.jpg\"]\n",
    "image_paths = \"violence_detection/Katharina_pytorch_Comparison/images_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'clip' has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vk/m4mvvjw97z740q4bfjr898nm0000gn/T/ipykernel_6403/2357949121.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ViT-L/14\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'clip' has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "# Load CLIP model\n",
    "model_name = \"ViT-L/14\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, preprocess = clip.load(model_name, device=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define safety-related text prompts\n",
    "text_prompts = [\n",
    "    \"a photo of a gun\", \"a photo of a shooting\", \"a photo of a weapon\",\n",
    "    \"a photo of a killing\", \"a photo of a bomb\", \"a photo of violence\",\n",
    "    \"a photo of other\", \"a photo of an animal\", \"a photo of a family\",\n",
    "    \"a photo of a meeting\", \"a photo of a tool\", \"a photo of a gathering\"\n",
    "]\n",
    "text_tokens = clip.tokenize(text_prompts).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute text embeddings\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens).to(torch.float32)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and evaluate the images\n",
    "final_sim = {}\n",
    "for image_path in image_paths:\n",
    "    try:\n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Compute image embedding\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image_input).to(torch.float32)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Compute similarity with text features\n",
    "        similarity = image_features @ text_features.T\n",
    "\n",
    "        # Store the results\n",
    "        final_sim[image_path] = similarity.cpu().numpy()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "torch.save(final_sim, \"final_res/sim_violence_test.torch\")\n",
    "\n",
    "# Print results\n",
    "for image_path, sim in final_sim.items():\n",
    "    print(f\"Similarity scores for {image_path}: {sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiwi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
