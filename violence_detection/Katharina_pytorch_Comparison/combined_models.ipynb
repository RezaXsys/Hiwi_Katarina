{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch datei vs json file vergelichen ist das möglich \n",
    "- HUggingfcae modell hinzufügen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details \n",
    "1. **CLIP-Modell:** Wird verwendet, um die Wahrscheinlichkeiten zu berechnen, ob ein Bild eine Waffe enthält. Der SChwellenwert ist anpassbar, falls die Klassifikationsergebnisse unzureichend sind.\n",
    "2. **Roboflow-Integration:** wird über die API initalisiert und für die Analyse von Bildern genutzt \n",
    "3. **Ergebnisvergleich:** Die Überschneidung und Unterschiede zwischen den Modellen werden als Mengenoperationen durchgeführt.\n",
    "4. **Ergebnisvergleich:** Die Überschneidungen und Unterschiede zwischen den Modellen werden als Mengenoperationen durchgeführt \n",
    "5. **Batch-Verarbeitung:** Bilder werden in Blöcken von 500 verarbeitet, um Speicherplatz effizient zu nutzen. \n",
    "6. **Dynamisch Anpassung:** Falls zu wenige Bilder Klassifiziert werden, wird der Schwellenwert dynamisch abgesenkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Laden der Daten: \n",
    "* Laden der klassifizieren Bilder mit torch-load aus der angebenen Datei -> Anmerkungen: es gibt eine json file, wo man die kennnummern und die Generierung und klassifiezierungen abspeichern kann\n",
    "    + Ghet schneller als die Bilder \n",
    "\n",
    "* Bilder immer Blockweiese (500) Verarbeiten \n",
    "### 2. Analyse mit dem CLIP-MODELL:\n",
    "* Verwendet ein CLIP-MODELL, um die Bildähnlichkeit zu bewerten und Überschneidungen zu finden. \n",
    "* Setzt einen Schwellenwert (z.B. 20%) und passe ihn dynamisch an, wenn nicht genügend Bilder Klassifiziert werden.\n",
    "### 3. Analyse mit Roboflow/Huggingface-Modell:\n",
    "* Führt eine weitere Analyse mit einem trainierten Roboflow-Modell (über Huggingface) durch. \n",
    "* Identifiziere Bilder mit Waffen basierend auf diesem Modell \n",
    "### 4. Überschneidungen und Statistiken:\n",
    "* Vergelicht die Ergebnisse von CLIP und dem Roboflow-Modell\n",
    "* Generiere eine Gegenüberstelllung der Anzhal der Bilder mit Waffen: \n",
    "    - In beiden Modellen Klassifiziert \n",
    "    - Nur in einem Modelle klassifiziert \n",
    "### 5. Bericht generieren: \n",
    "* Speichere die Ergebnisse und Statistiken in einer übersichtlichen Form (z.B. als CSV oder JSON)\n",
    "* Bilder die bei beiden Matchen sollen in einer separaten liste gespeichert werden. Schnittmenge im Detail überprüfen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel, pipeline\n",
    "from roboflow import Roboflow\n",
    "import json\n",
    "from PIL import Image \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SETTINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_THRESHOLD = 0.2\n",
    "BATCH_SIZE = 500\n",
    "ROBOWFLOW_API_KEY = \"your_roboflow_api_key\" # Bitte Gebenfalls eigenen API verwenden, Konto notwendig\n",
    "ROBOWFLOW_MODEL = \"your_model_version\" # TODO replace with model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Roboflow INIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf = Roboflow(api_key=ROBOWFLOW_API_KEY)\n",
    "#project = rf.workspace().project(\"your_project_name\")\n",
    "#roboflow_model = project.version(ROBOWFLOW_MODEL).model\n",
    "# TODO: replace with pretrained model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clip INIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **METHODS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(filepath):\n",
    "    # Images aus torch datei laden \n",
    "   return torch.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def load_roboflow_results(filepath):\n",
    "    # Ergebnisse aus der Roboflow datei laden und store detetcion per images \n",
    "#    with open(filepath, \"r\") as f:\n",
    "#        data = json.load(f)\n",
    "        \n",
    "#    roboflow_results = {}\n",
    "#    for entry in data: \n",
    "#        image_name = entry[\"image_name\"]\n",
    "#        detected_classes = entry[\"classes\"]\n",
    "        # List of detection weapon \n",
    "#        roboflow_results[image_name] = \"weapon\" if detected_classes else \"not weapon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_roboflow_results(filepath):\n",
    "    # Load JSON file and check for errors \n",
    "    with open(filepath, \"r\") as f: \n",
    "        try: \n",
    "            data = json.load(f) # Ensure proper JSON parsing\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            return {}\n",
    "        \n",
    "    # Ensure data is a list \n",
    "    if not isinstance(data, list): \n",
    "        print(\"Unexpected JSON format: Expected a list of sictionaries.\")\n",
    "        return {}\n",
    "    \n",
    "    roboflow_results = {}\n",
    "    for entry in data: \n",
    "        if not isinstance(entry, dict):\n",
    "            print(f\"Skipping invalid entry: {entry}\")\n",
    "            continue \n",
    "        image_name = entry.get(\"image_name\") # Use .get() to avoid KeyError\n",
    "        detected_classes = entry.get(\"classes\", [])  # Default to empty list if missing\n",
    "        \n",
    "        if not image_name: \n",
    "            print(f\"skipping entry with missing image_name: {entry}\")\n",
    "            continue \n",
    "    # If classes list is not empty, label as \"weapon\", otherwise \"not weapon\"\n",
    "        roboflow_results[image_name] = \"weapon\" if detected_classes else \"not weapon\"\n",
    "    return roboflow_results\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_roboflow(images):\n",
    "    \"roboflow Bilder\"\n",
    "    weapon_images = []\n",
    "    for image in images: \n",
    "        prediction = roboflow_model.predict(image, hosted = True )\n",
    "        if \"weapon\" in prediction[\"predictions\"]:# das muss an die Roboflow Modelle und ergebnisse angepasst werden\n",
    "            weapon_images.append(image)\n",
    "    return weapon_images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_clip(images, threshold): \n",
    "    # Analyze Bilder mit CLIP-MODELL\n",
    "    results_per_threshold = {t: [] for t in threshold}\n",
    "    \n",
    "    for image in images:\n",
    "        image_name = image.split(\"/\")[-1]\n",
    "        \n",
    "        inputs = clip_processor(text=[\"weapon\", \"not weapon\"],images=image, return_tensors=\"pt\", padding= True)\n",
    "        outputs = clip_model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image.softmax(dim=1).detach().numpy()\n",
    "        \n",
    "        for t in threshold: \n",
    "            results_per_threshold[t][image_name]= \"weapon\" if logits_per_image[0][0] > t else \"not weapon\"\n",
    "            \n",
    "        \n",
    "    return results_per_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **COMPARISONS**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(clip_results, roboflow_results):\n",
    "    # Vergeliche Ergebnisse und erstelle Statistiken\n",
    "    set_clip = set(clip_results)\n",
    "    set_roboflow = set(roboflow_results)\n",
    "    \n",
    "    overlap = set_clip.intersection(set_roboflow)\n",
    "    only_clip = set_clip.difference(set_roboflow)\n",
    "    only_roboflow = set_roboflow.difference(set_clip)\n",
    "    \n",
    "    return {\n",
    "        \"overlap\": len(overlap),\n",
    "        \"only_clip\": len(only_clip),\n",
    "        \"only_roboflow\": len(only_roboflow),\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results_second( clip_results, roboflow_results):\n",
    "    # Compare CLIP and Roboflow classifictaion per threshold\n",
    "    comparison_stats = {}\n",
    "    \n",
    "    for t, clip_classifications in clip_results.items():\n",
    "        correct = 0\n",
    "        total = len(roboflow_results)\n",
    "        false_positives = 0\n",
    "        false_negatives = 0 \n",
    "        \n",
    "        for image_name, roboflow_label in roboflow_results.items():\n",
    "            clip_label = clip_classifications.get(image_name, \"not weapon\")  # Default to \"not weapon\" if missing\n",
    "            \n",
    "            if clip_label == roboflow_label:\n",
    "                correct += 1 \n",
    "            elif clip_label == \"weapon\" and roboflow_label == \"not weapon\":\n",
    "                false_postives += 1 \n",
    "            elif clip_label == \"not weapon\" and roboflow_label == \"weapon\":\n",
    "                false_negatives += 1 \n",
    "                \n",
    "         # Calculate precision, recall and F1 score\n",
    "        precision = correct / (correct + false_positives) if ( correct + false_positives)> 0 else 0\n",
    "        recall = correct / (correct + false_negatives) if (correct + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        comparison_stats[t] = {\n",
    "            \"correct\": correct,\n",
    "            \"false_positives\": false_positives,\n",
    "            \"false_negatives\": false_negatives,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1_score\n",
    "        }\n",
    "        \n",
    "    return comparison_stats           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def main(): \n",
    "#   image_filepath = \"/ceph/lprasse/ClimateVisions/Tweet_Dataset/nsfw_cleaning/final_res/sim_violence_4.torch\"\n",
    "#   images = load_images(image_filepath)\n",
    "#   \n",
    "#   for i in range(0, len(images), BATCH_SIZE):\n",
    "#       batch = images[i:i + BATCH_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Loading images...\")\n",
    "    image_filepath = \"sim_violence_test.torch\"\n",
    "    images = load_images(image_filepath)\n",
    "    print(f\"Loaded {len(images)} images.\")\n",
    "\n",
    "    print(\"Loading Roboflow results...\")\n",
    "    roboflow_results = load_roboflow_results(\"weapon_classification_results.json\")\n",
    "    print(f\"Loaded {len(roboflow_results)} results.\")\n",
    "\n",
    "    thresholds = [i * 0.1 for i in range(1, 10)]\n",
    "    print(f\"Testing thresholds: {thresholds}\")\n",
    "\n",
    "    print(\"Analyzing images with CLIP model...\")\n",
    "    clip_results_per_threshold = analyze_with_clip(images, thresholds)\n",
    "\n",
    "    print(\"Comparing CLIP and Roboflow results...\")\n",
    "    comparison_stats = compare_results_second(clip_results_per_threshold, roboflow_results)\n",
    "    \n",
    "    if not comparison_stats:\n",
    "        print(\"Error: No comparison stats generated!\")\n",
    "        return  # Stop execution if results are missing\n",
    "\n",
    "    best_threshold = max(comparison_stats, key=lambda t: comparison_stats[t][\"f1_score\"])\n",
    "    best_f1 = comparison_stats[best_threshold][\"f1_score\"]\n",
    "\n",
    "    print(f\"Best threshold: {best_threshold} with F1-score: {best_f1}\")\n",
    "    print(json.dumps(comparison_stats, indent=4))\n",
    "\n",
    "    # Save results\n",
    "    print(\"Saving results to JSON...\")\n",
    "    with open(\"comparison_results.json\", \"w\") as f:\n",
    "        json.dump(comparison_stats, f, indent=4)\n",
    "    \n",
    "    print(\"File saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images...\n",
      "Loaded 60 images.\n",
      "Loading Roboflow results...\n",
      "Unexpected JSON format: Expected a list of sictionaries.\n",
      "Loaded 0 results.\n",
      "Testing thresholds: [0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9]\n",
      "Analyzing images with CLIP model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or jax.ndarray, but got <class 'str'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[43], line 15\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting thresholds: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthresholds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzing images with CLIP model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m clip_results_per_threshold \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_with_clip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComparing CLIP and Roboflow results...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m comparison_stats \u001b[38;5;241m=\u001b[39m compare_results_second(clip_results_per_threshold, roboflow_results)\n",
      "Cell \u001b[0;32mIn[42], line 8\u001b[0m, in \u001b[0;36manalyze_with_clip\u001b[0;34m(images, threshold)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m      6\u001b[0m     image_name \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mclip_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweapon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnot weapon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m clip_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     10\u001b[0m     logits_per_image \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits_per_image\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env-hiwi/lib/python3.9/site-packages/transformers/models/clip/processing_clip.py:109\u001b[0m, in \u001b[0;36mCLIPProcessor.__call__\u001b[0;34m(self, text, images, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mimage_processor_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mpixel_values\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env-hiwi/lib/python3.9/site-packages/transformers/image_processing_utils.py:41\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env-hiwi/lib/python3.9/site-packages/transformers/models/clip/image_processing_clip.py:286\u001b[0m, in \u001b[0;36mCLIPImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m do_convert_rgb \u001b[38;5;241m=\u001b[39m do_convert_rgb \u001b[38;5;28;01mif\u001b[39;00m do_convert_rgb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_convert_rgb\n\u001b[1;32m    284\u001b[0m validate_kwargs(captured_kwargs\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mkeys(), valid_processor_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_processor_keys)\n\u001b[0;32m--> 286\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mmake_list_of_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid_images(images):\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env-hiwi/lib/python3.9/site-packages/transformers/image_utils.py:206\u001b[0m, in \u001b[0;36mmake_list_of_images\u001b[0;34m(images, expected_ndims)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    202\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image shape. Expected either \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_ndims\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_ndims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, but got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m         )\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m images\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax.ndarray, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or jax.ndarray, but got <class 'str'>."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiwi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
