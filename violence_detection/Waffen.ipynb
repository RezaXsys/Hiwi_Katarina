{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS \n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDEE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgangssituation: \n",
    "Twitter datensatz, Bilder sind als links zur Verfügung können aber runtergeladen werden \n",
    "\n",
    "=> Es sollen nur Bilder rausgefiltert werden, die tatsächlich Waffen in gewaltätigen Kontext zeigen. Bilder auf denen Waffen nicht eingestezt werden, können drin bleiben um die Representation von Solchen Bildmaterial zu gewährleisten. \n",
    "Programmierspraceh: Python \n",
    "-> zwei Schritte: \n",
    "1. Alle Bilder mit Waffen rausfiltern \n",
    "    * YOLOv8, \n",
    "2. Dann mit Pose-Estimation entscheiden auf welchen Bildern Aggresives Handlen zu erkennen ist \n",
    "    * je nach größe der Ergebnissemeng, Bilder manuel aussortieren\n",
    "    * OpenPose "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erste Version zur Umsetzung "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 1: Bilder mit Waffen erkennen \n",
    "1. Laden der YOLOv8-Modelldateien:\n",
    "    * Yolov8 kann man mit Pytorch oder der ultralytics Bibliotehl verwenden \n",
    "2. Bilder asu dem Dataset herunteralden:\n",
    "    Die Bild-Links aus dem Twitter-Datensatz werden heruntergeladen un dlokal gespeichert \n",
    "3. Waffen erkennen:\n",
    "    * YOLOv8 wird verwendet, um Bilder zu klassifizieren, die Waffen enthalten "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 2:  Erkennung aggresiver Handlungen \n",
    "1. Pose-Estimation mit Open-Pose:\n",
    "    * OpenPose wird angewandt, um die Körperhaltung und Bewegungen der Personen im Bild erkennen \n",
    "    * Erkennungsmerkmale für aggresuve Handlungen werden analysiert, z.B. Gesten die auf Gewalt hindeuten \n",
    "2. Filtern von Ergebnissen:\n",
    "    * Bilder, die als aggressiv eingestuft werden, werden separiert.\n",
    "3. Optionale manuelle Nachprüfung: \n",
    "    * Je nach Größe der Ergebnisdatenmenge können die Bilder visuell überprüft werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordner für Bilder \n",
    "IMAGE_DIR = \"./images\"\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Bilder herunterladen \n",
    "def download_images(image_links):\n",
    "    downloaded_images = []\n",
    "    for i, link in enumerate(image_links):\n",
    "        try:\n",
    "            response = requests.get(link, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                image_path = os.path.join(IMAGE_DIR, f\"image_{i}.jpg\")\n",
    "                with open(image_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                downloaded_images.append(image_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {link}: {e}\")\n",
    "    return downloaded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. YOLOv8: Erkennung von Waffen\n",
    "def detect_weapons(image_paths, model_path=\"yolov8n.pt\"):\n",
    "    model = YOLO(model_path)  # YOLOv8-Modell laden\n",
    "    weapon_images = []\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        results = model(image_path)\n",
    "        # Prüfen, ob eine Waffe im Bild erkannt wurde\n",
    "        if any(detection['name'] == 'weapon' for detection in results[0].boxes.data):\n",
    "            # detection results of images analyzed \n",
    "            # resulst[0].boxes.data cinatins all the bounding boxes deteced in the image, along with metadata such as the class name (name), condfidence score, and coordinates\n",
    "            # the function checks if any deteced object in the image has the class name 'weapon'. This indicates the presence of a weapon in the image \n",
    "            weapon_images.append(image_path)\n",
    "    return weapon_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Object Detection Capabilities:\n",
    "    * is a state-of-the_art object detection algorithm optimized for detecting objects in real-time with high accuracy \n",
    "    * its architecture uses convolutional neurla networks (CNN) to identify patterns in images and classify objetcs baseed on those patterns \n",
    "    * is preptrained on diverese datasets, including  images with weapons, makng it suitabe for this task\n",
    "2. Efficient Processing:\n",
    "    * The grid-based detection systems in YOLO ensures that the entire image is analyzed efficiently in a single forward pass, making it fast and scalable for multiple images \n",
    "3. Selective Filtering: \n",
    "    * The function filters results based on the class name weapon. This ensures that only relevnat images are falgged, reducing the noise in subsequent steps \n",
    "4. Flexibility: \n",
    "    * By using file paths, the function is modular and can easily be adapated for different datasets or integrated into larger piplines \n",
    "    * the model_path argument allows switching between differnet YOLOv8 models ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion zur Erkennung aggressiver Posen\n",
    "def is_aggressive_pose(pose_data):\n",
    "    # Placeholder für Logik zur Aggressions-Erkennung\n",
    "    # z.B. erhobene Hände/Waffen, gebeugte Haltungen etc. ===> Open Pose Datei\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. OpenPose: Erkennung aggressiver Posen\n",
    "def analyze_aggressive_poses(image_paths, openpose_model_path=\"./openpose/models/\"):\n",
    "    aggressive_images = []\n",
    "    # OpenPose initialisieren\n",
    "    params = {\"model_folder\": openpose_model_path}\n",
    "    openpose = cv2.dnn.readNetFromCaffe(params[\"prototxt\"], params[\"caffemodel\"])\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        image = cv2.imread(image_path)\n",
    "        h, w, _ = image.shape\n",
    "        blob = cv2.dnn.blobFromImage(image, 1.0 / 255, (368, 368), (0, 0, 0), swapRB=False, crop=False)\n",
    "        openpose.setInput(blob)\n",
    "        output = openpose.forward()\n",
    "\n",
    "        # Analyse der Pose-Daten (z. B. erkennbare Aggression)\n",
    "        if is_aggressive_pose(output):\n",
    "            aggressive_images.append(image_path)\n",
    "    return aggressive_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hauptfunktion\n",
    "def main(image_links):\n",
    "    print(\"1. Bilder herunterladen...\")\n",
    "    images = download_images(image_links)\n",
    "    \n",
    "    print(\"2. Waffen erkennen...\")\n",
    "    weapon_images = detect_weapons(images)\n",
    "    \n",
    "    print(\"3. Aggressive Posen analysieren...\")\n",
    "    aggressive_images = analyze_aggressive_poses(weapon_images)\n",
    "    \n",
    "    print(f\"{len(aggressive_images)} aggressive Bilder gefunden.\")\n",
    "    return aggressive_images\n",
    "\n",
    "# Bild-URLS \n",
    "image_links = [\"http://example.com/image1.jpg\", \"http://example.com/image2.jpg\"]\n",
    "aggressive_images = main(image_links)\n",
    "\n",
    "# Ergebnis ausgeben\n",
    "print(\"Aggressive Bilder:\", aggressive_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bereingng des Datensatzes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenssatzverarbeitung \n",
    "Laden, Anzeigen und Sortieren von Bildern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"path/to/your/images\"\n",
    "for file in os.listdir(dataset_path):\n",
    "    img_path = os.path.join(dataset_path, file)\n",
    "    img = cv2.imread(img_path)\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    key = cv2.waitKey(0)  # Sortieren mit Tasten, z.B. 'd' für delete\n",
    "    if key == ord('d'):\n",
    "        os.remove(img_path)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waffenerkennung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PAPER\n",
    "\n",
    "* Liu, Z., Xue, Z., Blum, R.S. et al. Concealed weapon detection and visualization in a synthesized image. Pattern Anal Applic 8, 375–389 (2006). https://doi.org/10.1007/s10044-005-0020-8\n",
    "    - Bilder von heterogenen Bildsensoren liefern zusätzliche Informationen über die Szene. Das visuelle Bild identifiziert Personen, das Infrarot- oder Millimeterwellenbild erkennt verdächtige Bereiche mit Waffen. \n",
    "    - Die Informationen aus Multisensor-Bildern werden normalerweise mit der Technik der Multiresolution Pixel-Level Image Fusion integriert. Die Leistung von Multiresolution-Bildfusionsalgorithmen ist nicht immer zufriedenstellend, wenn die Bilder stark voneinander abweichen. In dieser Studie wird eine neue Strategie vorgeschlagen, die aus zwei Schritten besteht. Im ersten Schritt wird ein unüberwachtes Fuzzy-k-Means-Clustering verwendet.\n",
    "    - Im zweiten Schritt wird der erkannte Bereich in das visuelle Bild integriert, wobei eine Mosaiktechnik mit mehreren Auflösungen verwendet wird. So wird das synthetisierte Bild verbessert, während der Bereich der verborgenen Waffe hervorgehoben wird. Die experimentellen Ergebnisse zeigen die Wirksamkeit des Ansatzes.\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Open_Source_Modell: YOLO\n",
    "    - https://docs.ultralytics.com/de/models/yolov8/\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
