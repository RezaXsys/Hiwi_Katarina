{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch datei vs json file vergelichen ist das möglich \n",
    "- HUggingfcae modell hinzufügen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details \n",
    "1. **CLIP-Modell:** Wird verwendet, um die Wahrscheinlichkeiten zu berechnen, ob ein Bild eine Waffe enthält. Der SChwellenwert ist anpassbar, falls die Klassifikationsergebnisse unzureichend sind.\n",
    "2. **Roboflow-Integration:** wird über die API initalisiert und für die Analyse von Bildern genutzt \n",
    "3. **Ergebnisvergleich:** Die Überschneidung und Unterschiede zwischen den Modellen werden als Mengenoperationen durchgeführt.\n",
    "4. **Ergebnisvergleich:** Die Überschneidungen und Unterschiede zwischen den Modellen werden als Mengenoperationen durchgeführt \n",
    "5. **Batch-Verarbeitung:** Bilder werden in Blöcken von 500 verarbeitet, um Speicherplatz effizient zu nutzen. \n",
    "6. **Dynamisch Anpassung:** Falls zu wenige Bilder Klassifiziert werden, wird der Schwellenwert dynamisch abgesenkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Laden der Daten: \n",
    "* Laden der klassifizieren Bilder mit torch-load aus der angebenen Datei -> Anmerkungen: es gibt eine json file, wo man die kennnummern und die Generierung und klassifiezierungen abspeichern kann\n",
    "    + Ghet schneller als die Bilder \n",
    "\n",
    "* Bilder immer Blockweiese (500) Verarbeiten \n",
    "### 2. Analyse mit dem CLIP-MODELL:\n",
    "* Verwendet ein CLIP-MODELL, um die Bildähnlichkeit zu bewerten und Überschneidungen zu finden. \n",
    "* Setzt einen Schwellenwert (z.B. 20%) und passe ihn dynamisch an, wenn nicht genügend Bilder Klassifiziert werden.\n",
    "### 3. Analyse mit Roboflow/Huggingface-Modell:\n",
    "* Führt eine weitere Analyse mit einem trainierten Roboflow-Modell (über Huggingface) durch. \n",
    "* Identifiziere Bilder mit Waffen basierend auf diesem Modell \n",
    "### 4. Überschneidungen und Statistiken:\n",
    "* Vergelicht die Ergebnisse von CLIP und dem Roboflow-Modell\n",
    "* Generiere eine Gegenüberstelllung der Anzhal der Bilder mit Waffen: \n",
    "    - In beiden Modellen Klassifiziert \n",
    "    - Nur in einem Modelle klassifiziert \n",
    "### 5. Bericht generieren: \n",
    "* Speichere die Ergebnisse und Statistiken in einer übersichtlichen Form (z.B. als CSV oder JSON)\n",
    "* Bilder die bei beiden Matchen sollen in einer separaten liste gespeichert werden. Schnittmenge im Detail überprüfen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vk/m4mvvjw97z740q4bfjr898nm0000gn/T/ipykernel_35563/4187383440.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCLIPProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIPModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mroboflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRoboflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel, pipeline\n",
    "from roboflow import Roboflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SETTINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_THRESHOLD = 0.2\n",
    "BATCH_SIZE = 500\n",
    "ROBOWFLOW_API_KEY = \"your_roboflow_api_key\" # Bitte Gebenfalls eigenen API verwenden, Konto notwendig\n",
    "ROBOWFLOW_MODEL = \"your_model_version\" # TODO replace with model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Roboflow INIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = Roboflow(api_key=ROBOWFLOW_API_KEY)\n",
    "project = rf.workspace().project(\"your_project_name\")\n",
    "roboflow_model = project.version(ROBOWFLOW_MODEL).model\n",
    "# TODO: replace with pretrained model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clip INIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **METHODS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(filepath):\n",
    "    # Images aus torch datei laden \n",
    "    return torch.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_roboflow(images):\n",
    "    \"roboflow Bilder\"\n",
    "    weapon_images = []\n",
    "    for image in images: \n",
    "        prediction = roboflow_model.predict(image, hosted = True )\n",
    "        if \"weapon\" in prediction[\"predictions\"]:# das muss an die Roboflow Modelle und ergebnisse angepasst werden\n",
    "            weapon_images.append(image)\n",
    "    return weapon_images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_clip(images, threshold): \n",
    "    # Analyze Bilder mit CLIP-MODELL\n",
    "    weapon_images = []\n",
    "    for image in images: \n",
    "        inputs = clip_processor(text=[\"weapon\", \"not weapon\"], images= image, return_tensors=\"pt\", padding=True)\n",
    "        outputs = clip_model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image.softmax(dim=1).detach().numpy()\n",
    "        \n",
    "        if logits_per_image[0][0] > threshold: # this defines the \"weapon\" confidence \n",
    "            weapon_images.append(image)\n",
    "    return weapon_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **COMPARISONS**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(clip_results, roboflow_results):\n",
    "    # Vergeliche Ergebnisse und erstelle Statistiken\n",
    "    set_clip = set(clip_results)\n",
    "    set_roboflow = set(roboflow_results)\n",
    "    \n",
    "    overlap = set_clip.intersection(set_roboflow)\n",
    "    only_clip = set_clip.difference(set_roboflow)\n",
    "    only_roboflow = set_roboflow.difference(set_clip)\n",
    "    \n",
    "    return {\n",
    "        \"overlap\": len(overlap),\n",
    "        \"only_clip\": len(only_clip),\n",
    "        \"only_roboflow\": len(only_roboflow),\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(): \n",
    "    image_filepath = \"/ceph/lprasse/ClimateVisions/Tweet_Dataset/nsfw_cleaning/final_res/sim_violence_4.torch\"\n",
    "    images = load_images(image_filepath)\n",
    "    \n",
    "    for i in range(0, len(images), BATCH_SIZE):\n",
    "        batch = images[i:i + BATCH_SIZE]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiwi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
