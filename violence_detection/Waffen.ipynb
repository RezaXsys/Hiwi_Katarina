{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS \n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDEE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgangssituation: \n",
    "Twitter datensatz, Bilder sind als links zur Verfügung können aber runtergeladen werden \n",
    "\n",
    "=> Es sollen nur Bilder rausgefiltert werden, die tatsächlich Waffen in gewaltätigen Kontext zeigen. Bilder auf denen Waffen nicht eingestezt werden, können drin bleiben um die Representation von Solchen Bildmaterial zu gewährleisten. \n",
    "Programmierspraceh: Python \n",
    "-> zwei Schritte: \n",
    "1. Alle Bilder mit Waffen rausfiltern \n",
    "    * YOLOv8, \n",
    "2. Dann mit Pose-Estimation entscheiden auf welchen Bildern Aggresives Handlen zu erkennen ist \n",
    "    * je nach größe der Ergebnissemeng, Bilder manuel aussortieren\n",
    "    * OpenPose "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erste Version zur Umsetzung "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 1: Bilder mit Waffen erkennen \n",
    "1. Laden der YOLOv8-Modelldateien:\n",
    "    * Yolov8 kann man mit Pytorch oder der ultralytics Bibliotehl verwenden \n",
    "2. Bilder asu dem Dataset herunteralden:\n",
    "    Die Bild-Links aus dem Twitter-Datensatz werden heruntergeladen un dlokal gespeichert \n",
    "3. Waffen erkennen:\n",
    "    * YOLOv8 wird verwendet, um Bilder zu klassifizieren, die Waffen enthalten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Set up \n",
    "TRAINED_MODEL_PATH = \"./yolov8_weapon_model.pt\" # Pfad um trainiertes Modell zu speichern \n",
    "\n",
    "# Ordner für Bilder \n",
    "IMAGE_DIR = \"./images\"\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "def download_training_dataset():\n",
    "    rf = Roboflow(api_key=\"rf_fddSrqzakrgR3TlEVlzw0p4BNKx1\")\n",
    "    project = rf.workspace(\"joseph-nelson\").project(\"pistols\")\n",
    "    version = project.version(1)\n",
    "    dataset = version.download(\"yolov8\")\n",
    "    return dataset.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train the YOLOv8 Model\n",
    "def train_yolo_model(dataset_path, epochs=10, batch_size=16):\n",
    "    model = YOLO(\"yolov8n.pt\")  # Load YOLOv8 pre-trained model\n",
    "    model.train(data=os.path.join(dataset_path, \"data.yaml\"), epochs=epochs, batch=batch_size)\n",
    "    model.export(format=\"pt\")  # Save the trained model\n",
    "    model.save(TRAINED_MODEL_PATH)\n",
    "    return TRAINED_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Bilder herunterladen \n",
    "def download_images(image_links):\n",
    "    downloaded_images = []\n",
    "    for i, link in enumerate(image_links):\n",
    "        try:\n",
    "            response = requests.get(link, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                image_path = os.path.join(IMAGE_DIR, f\"image_{i}.jpg\")\n",
    "                with open(image_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                downloaded_images.append(image_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {link}: {e}\")\n",
    "    return downloaded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. YOLOv8: Erkennung von Waffen\n",
    "def detect_weapons(image_paths, model_path=\"yolov8n.pt\"):\n",
    "    model = YOLO(model_path)  # YOLOv8-Modell laden\n",
    "    weapon_images = []\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        results = model(image_path)\n",
    "        # Prüfen, ob eine Waffe im Bild erkannt wurde\n",
    "        if any(detection['name'] == 'weapon' for detection in results[0].boxes.data):\n",
    "            # detection results of images analyzed \n",
    "            # resulst[0].boxes.data cinatins all the bounding boxes deteced in the image, along with metadata such as the class name (name), condfidence score, and coordinates\n",
    "            # the function checks if any deteced object in the image has the class name 'weapon'. This indicates the presence of a weapon in the image \n",
    "            weapon_images.append(image_path)\n",
    "    return weapon_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_weapons(image_paths, model_path):\n",
    "    model = YOLO(model_path)  # Load the trained YOLOv8 model\n",
    "    weapon_images = []\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        results = model(image_path)  # Perform detection\n",
    "        # Check for weapons in the detection results\n",
    "        for detection in results[0].boxes.data:\n",
    "            class_id = int(detection[5])  # Class ID is typically in the 6th column\n",
    "            class_name = model.names[class_id]\n",
    "            print(model.names)\n",
    "            if class_name == 'weapon':  # Match with the desired class name\n",
    "                weapon_images.append(image_path)\n",
    "                break  # If one weapon is found, no need to check further\n",
    "\n",
    "    return weapon_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Downloading training dataset...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "{\n    \"error\": {\n        \"message\": \"This API key does not exist (or has been revoked).\",\n        \"status\": 401,\n        \"type\": \"OAuthException\",\n        \"hint\": \"You may retrieve your API key via the Roboflow Dashboard. Go to Account > Roboflow Keys to retrieve yours.\",\n        \"key\": \"rf_fddSrqzakrgR3TlEVlzw0p4BNKx1\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vk/m4mvvjw97z740q4bfjr898nm0000gn/T/ipykernel_33109/2497742122.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Execute the workflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mweapon_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Output results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vk/m4mvvjw97z740q4bfjr898nm0000gn/T/ipykernel_33109/2497742122.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(image_links)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Step 1: Downloading training dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_training_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Step 2: Training YOLOv8 model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vk/m4mvvjw97z740q4bfjr898nm0000gn/T/ipykernel_33109/761331747.py\u001b[0m in \u001b[0;36mdownload_training_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mroboflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRoboflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdownload_training_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRoboflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rf_fddSrqzakrgR3TlEVlzw0p4BNKx1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mproject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"joseph-nelson\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pistols\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hiwi_env/lib/python3.9/site-packages/roboflow/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, model_format, notebook)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnotebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monboarding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hiwi_env/lib/python3.9/site-packages/roboflow/__init__.py\u001b[0m in \u001b[0;36mauth\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"onboarding\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hiwi_env/lib/python3.9/site-packages/roboflow/__init__.py\u001b[0m in \u001b[0;36mcheck_key\u001b[0;34m(api_key, model, notebook, num_retries)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m401\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: {\n    \"error\": {\n        \"message\": \"This API key does not exist (or has been revoked).\",\n        \"status\": 401,\n        \"type\": \"OAuthException\",\n        \"hint\": \"You may retrieve your API key via the Roboflow Dashboard. Go to Account > Roboflow Keys to retrieve yours.\",\n        \"key\": \"rf_fddSrqzakrgR3TlEVlzw0p4BNKx1\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "# Main Function\n",
    "def main(image_links):\n",
    "    print(\"Step 1: Downloading training dataset...\")\n",
    "    dataset_path = download_training_dataset()\n",
    "\n",
    "    print(\"Step 2: Training YOLOv8 model...\")\n",
    "    trained_model_path = train_yolo_model(dataset_path, epochs=20, batch_size=16)\n",
    "\n",
    "    print(\"Step 3: Downloading test images...\")\n",
    "    test_images = download_images(image_links)\n",
    "\n",
    "    #print(\"Step 4: Detecting weapons in test images...\")\n",
    "    #weapon_images = detect_weapons(test_images, trained_model_path)\n",
    "    weapon_images = \"hellooo\"\n",
    "\n",
    "    #print(f\"Weapon detected in {len(weapon_images)} images.\")\n",
    "    return weapon_images\n",
    "\n",
    "# Test Image URLs TODO== actuall images from twitter data set \n",
    "image_links = [\"http://example.com/image1.jpg\", \"http://example.com/image2.jpg\"]\n",
    "\n",
    "# Execute the workflow\n",
    "weapon_images = main(image_links)\n",
    "\n",
    "# Output results\n",
    "print(\"Images with detected weapons:\", weapon_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 2:  Erkennung aggresiver Handlungen \n",
    "1. Pose-Estimation mit Open-Pose:\n",
    "    * OpenPose wird angewandt, um die Körperhaltung und Bewegungen der Personen im Bild erkennen \n",
    "    * Erkennungsmerkmale für aggresuve Handlungen werden analysiert, z.B. Gesten die auf Gewalt hindeuten \n",
    "2. Filtern von Ergebnissen:\n",
    "    * Bilder, die als aggressiv eingestuft werden, werden separiert.\n",
    "3. Optionale manuelle Nachprüfung: \n",
    "    * Je nach Größe der Ergebnisdatenmenge können die Bilder visuell überprüft werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Object Detection Capabilities:\n",
    "    * is a state-of-the_art object detection algorithm optimized for detecting objects in real-time with high accuracy \n",
    "    * its architecture uses convolutional neurla networks (CNN) to identify patterns in images and classify objetcs baseed on those patterns \n",
    "    * is preptrained on diverese datasets, including  images with weapons, makng it suitabe for this task\n",
    "2. Efficient Processing:\n",
    "    * The grid-based detection systems in YOLO ensures that the entire image is analyzed efficiently in a single forward pass, making it fast and scalable for multiple images \n",
    "3. Selective Filtering: \n",
    "    * The function filters results based on the class name weapon. This ensures that only relevnat images are falgged, reducing the noise in subsequent steps \n",
    "4. Flexibility: \n",
    "    * By using file paths, the function is modular and can easily be adapated for different datasets or integrated into larger piplines \n",
    "    * the model_path argument allows switching between differnet YOLOv8 models ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion zur Erkennung aggressiver Posen\n",
    "def is_aggressive_pose(pose_data):\n",
    "    # Placeholder für Logik zur Aggressions-Erkennung\n",
    "    # z.B. erhobene Hände/Waffen, gebeugte Haltungen etc. ===> Open Pose Datei\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. OpenPose: Erkennung aggressiver Posen\n",
    "def analyze_aggressive_poses(image_paths, openpose_model_path=\"./openpose/models/\"):\n",
    "    # TODO: OpenPose-Modeel laden kontrollieren ob Nividia GPU support mölich ist \n",
    "    # https://github.com/CMU-Perceptual-Computing-Lab/openpose_train/blob/master/experimental_models/README.md\n",
    "    aggressive_images = []\n",
    "    # OpenPose initialisieren\n",
    "    params = {\"model_folder\": openpose_model_path}\n",
    "    openpose = cv2.dnn.readNetFromCaffe(params[\"prototxt\"], params[\"caffemodel\"])\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        image = cv2.imread(image_path)\n",
    "        h, w, _ = image.shape\n",
    "        # TODO: Anpassen was für Bildgrößen und wie das genau noch aussehen kann \n",
    "        blob = cv2.dnn.blobFromImage(image, 1.0 / 255, (368, 368), (0, 0, 0), swapRB=False, crop=False)\n",
    "        openpose.setInput(blob)\n",
    "        output = openpose.forward()\n",
    "\n",
    "        # Analyse der Pose-Daten (z. B. erkennbare Aggression)\n",
    "        if is_aggressive_pose(output):\n",
    "            aggressive_images.append(image_path)\n",
    "    return aggressive_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bereingng des Datensatzes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenssatzverarbeitung \n",
    "Laden, Anzeigen und Sortieren von Bildern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"path/to/your/images\"\n",
    "for file in os.listdir(dataset_path):\n",
    "    img_path = os.path.join(dataset_path, file)\n",
    "    img = cv2.imread(img_path)\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    key = cv2.waitKey(0)  # Sortieren mit Tasten, z.B. 'd' für delete\n",
    "    if key == ord('d'):\n",
    "        os.remove(img_path)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waffenerkennung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PAPER\n",
    "\n",
    "* Liu, Z., Xue, Z., Blum, R.S. et al. Concealed weapon detection and visualization in a synthesized image. Pattern Anal Applic 8, 375–389 (2006). https://doi.org/10.1007/s10044-005-0020-8\n",
    "    - Bilder von heterogenen Bildsensoren liefern zusätzliche Informationen über die Szene. Das visuelle Bild identifiziert Personen, das Infrarot- oder Millimeterwellenbild erkennt verdächtige Bereiche mit Waffen. \n",
    "    - Die Informationen aus Multisensor-Bildern werden normalerweise mit der Technik der Multiresolution Pixel-Level Image Fusion integriert. Die Leistung von Multiresolution-Bildfusionsalgorithmen ist nicht immer zufriedenstellend, wenn die Bilder stark voneinander abweichen. In dieser Studie wird eine neue Strategie vorgeschlagen, die aus zwei Schritten besteht. Im ersten Schritt wird ein unüberwachtes Fuzzy-k-Means-Clustering verwendet.\n",
    "    - Im zweiten Schritt wird der erkannte Bereich in das visuelle Bild integriert, wobei eine Mosaiktechnik mit mehreren Auflösungen verwendet wird. So wird das synthetisierte Bild verbessert, während der Bereich der verborgenen Waffe hervorgehoben wird. Die experimentellen Ergebnisse zeigen die Wirksamkeit des Ansatzes.\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Open_Source_Modell: YOLO\n",
    "    - https://docs.ultralytics.com/de/models/yolov8/\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiwi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
